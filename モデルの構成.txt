軍儀（Gungi）のような**複雑なボードゲーム**を扱う場合，AIモデルの選定は学習効率・推論速度・戦略性のバランスを取ることが重要です．
ここでは，あなたの目的（＝**遊べるレベルの敵AIをWebで動かしたい**）に合わせて，段階的に「どのモデルを使うか」を整理します．

---

## 🎯 まず結論：使うべきモデル構成

最終的には **AlphaZero型の「方策＋価値ネットワーク（Policy-Value Network）」** を採用するのが最もバランスが良いです．

---

## 🧩 モデル構成概要

### ✅ 方策＋価値ネット（Policy-Value Network）

| 出力              | 役割             | 出力の意味                 |
| --------------- | -------------- | --------------------- |
| **Policy head** | 次の手をどのくらい選ぶべきか | 各合法手に対する確率分布（softmax） |
| **Value head**  | 現在の局面の価値       | 「勝つ確率」を -1～+1 の範囲で出力  |

学習データは自己対戦で得られる `(盤面, 方策π, 勝敗z)` の三つ組です．
これを使って，**深層強化学習（AlphaZero方式）**を行います．

---

## 🧠 モデルアーキテクチャの具体例

### ベース：小型ResNet（畳み込みニューラルネット）

軍儀は将棋より盤面が広く・駒の種類も多いため，
**局所的特徴（周囲の駒）を捉えるCNN**が適しています．

```python
# 概要構成
Input: (C, N, N)   # C = チャンネル（駒情報・手番など）
↓
ResNetブロック × 5〜10層
↓
分岐
  ├─ Policy head → Conv2d → Flatten → Softmax over all moves
  └─ Value head  → Dense → tanh（出力は -1〜+1）
```

* **入力（盤面表現）**

  * N×Nマスの各マスを，チャンネル方向に「駒の種類 × サイド」でone-hotエンコード
  * 例：64種類の駒 → 64チャネル × 9×9 など
* **ResNetの特徴**

  * skip connection により深い学習が安定
  * AlphaZeroでも採用されている構造

---

## ⚙️ 推論時の流れ（MCTS＋NN）

1. 現局面を入力 → ネットワークから `(policy, value)` を取得
2. モンテカルロ木探索（MCTS）で複数手をシミュレーション
3. Policy確率で最も良い手を選択して返す

Webで動かすときは，

* MCTSの探索回数を減らす（例：100〜200）
* または「policyの最大値」をそのまま選ぶ簡易モードも用意

---

## 🚀 段階的戦略（軽量モデル → 本格モデル）

| 段階              | モデル                 | 概要          | 学習目的      |
| --------------- | ------------------- | ----------- | --------- |
| **Phase 1（試作）** | 3層CNN＋全結合           | 軽量・学習確認用    | とりあえず動作確認 |
| **Phase 2（改良）** | 5〜8層ResNet          | 軍儀簡易ルールで安定化 | 初期学習を回す   |
| **Phase 3（本格）** | 10層ResNet＋BatchNorm | 公式ルール・自己対戦  | 実戦レベルへ    |

---

## 🧮 学習時の損失関数

[
L = (z - v)^2 - π^T \log(p)
]

* ( v )：モデルの出力（価値）
* ( z )：実際の勝敗（1 or -1）
* ( p )：モデル出力の方策
* ( π )：MCTSで得られた実際の方策

👉 **価値誤差 + 方策誤差** の合計を最小化するシンプルな構造です．

---

## ⚡ Web公開時に向くモデル形式

| 形式                | 内容               | 特徴               |
| ----------------- | ---------------- | ---------------- |
| **PyTorch (.pt)** | 標準的な学習・保存形式      | 学習フェーズに使用        |
| **ONNX (.onnx)**  | 推論高速化形式          | WebAPIやブラウザで利用可能 |
| **TensorRT**      | GPU推論用（NVIDIA環境） | Jetsonなどでも実用的    |

→ 学習後はONNXに変換して，FastAPIで推論できるようにするのが最適です．

---

## 🧠 まとめ

| 要素       | 推奨                     |
| -------- | ---------------------- |
| モデルタイプ   | Policy-Value Network   |
| ベース構造    | 小型ResNet（5〜10層）        |
| 出力       | 方策分布＋価値スカラー            |
| 学習方式     | AlphaZero式の自己対戦強化学習    |
| 推論形式     | ONNXモデル＋FastAPIエンドポイント |
| 推論アルゴリズム | MCTS（探索回数100〜400）      |

---


